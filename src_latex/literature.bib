@misc{d6d98d35ebcc5cdc95330ce821728ac3d23d8292,
  author   = {O. Sallam and Mirjam Furth},
  title    = {Inference of water waves surface elevation from horizontal velocity components using physics informed neural networks (PINN)},
  journal  = {Unknown Journal},
  year     = {2024},
  abstract = {In this paper, a mathematical model is presented to infer the wave free surface elevation from the horizontal velocity components using Physics Informed Neural Network (PINN). PINN is a deep learning framework to solve forward and inverse Ordinary/Partial Differential Equations (ODEs/PDEs). The model is verified by measuring a numerically generated Kelvin waves downstream of a KRISO Container Ship (KCS). The KCS Kelvin waves are generated using two phase Volume of Fluid (VoF) Computational Fluid Dynamics (CFD) simulation with OpenFOAM. In addition, the paper presented the use of the Fourier Features decomposition of the Neural Network inputs to avoid the spectral bias phenomena; Spectral bias is the tendency of Neural Network to converge towards the low frequency solution faster than the high frequency one. Fourier Features decomposition layer showed an improvement for the model learning, as the model was able to learn the high and low frequency components simultaneously.}
}

@misc{7fcd4b3c875d8e41eb0c184aa1a42bf4c8906d61,
  author   = {Raphael Leiteritz and Marcel Hurler and D. Pflüger},
  title    = {Learning Free-Surface Flow with Physics-Informed Neural Networks},
  journal  = {2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  year     = {2021},
  doi      = {10.1109/ICMLA52953.2021.00266},
  url      = {https://doi.org/10.1109/ICMLA52953.2021.00266},
  pages    = {1668-1673},
  abstract = {The interface between data-driven learning methods and classical simulation poses an interesting field offering a multitude of new applications. In this work, we build on the notion of physics-informed neural networks (PINNs) and employ them in the area of shallow-water equation (SWE) models. These models play an important role in modeling and simulating free-surface flow scenarios such as in flood-wave propagation or tsunami waves. Different formulations of the PINN residual are compared to each other and multiple optimizations are being evaluated to speed up the convergence rate. We test these with different 1-D and 2-D experiments and finally demonstrate that regarding a SWE scenario with varying bathymetry, the method is able to produce competitive results in comparison to the direct numerical simulation with a total relative L2 error of 8.9e−3.}
}

@misc{0fc26db616856a9dea2be1597b21842d80c45f54,
  author   = {Rui Gao and R. Jaiman},
  title    = {H-SIREN: Improving implicit neural representations with hyperbolic periodic functions},
  journal  = {ArXiv},
  year     = {2024},
  doi      = {10.48550/arXiv.2410.04716},
  volume   = {abs/2410.04716},
  abstract = {Implicit neural representations (INR) have been recently adopted in various applications ranging from computer vision tasks to physics simulations by solving partial differential equations. Among existing INR-based works, multi-layer perceptrons with sinusoidal activation functions find widespread applications and are also frequently treated as a baseline for the development of better activation functions for INR applications. Recent investigations claim that the use of sinusoidal activation functions could be sub-optimal due to their limited supported frequency set as well as their tendency to generate over-smoothed solutions. We provide a simple solution to mitigate such an issue by changing the activation function at the first layer from \$\\sin(x)\$ to \$\\sin(\\sinh(2x))\$. We demonstrate H-SIREN in various computer vision and fluid flow problems, where it surpasses the performance of several state-of-the-art INRs.}
}

@misc{0d752c79fb816703274a3d37f85a85689a2a9405,
  author   = {Sourav Mishra and Shreya Hallikeri and Suresh Sundaram},
  title    = {REAct: Rational Exponential Activation for Better Learning and Generalization in PINNs},
  journal  = {Unknown Journal},
  year     = {2025},
  doi      = {10.1109/ICASSP49660.2025.10887788},
  url      = {https://doi.org/10.1109/ICASSP49660.2025.10887788},
  abstract = {Physics-Informed Neural Networks (PINNs) offer a promising approach to simulating physical systems. Still, their application is limited by optimization challenges, mainly due to the lack of activation functions that generalize well across several physical systems. Existing activation functions often lack such flexibility and generalization power. To address this issue, we introduce Rational Exponential Activation (REAct), a generalized form of tanh consisting of four learnable shape parameters. Experiments show that REAct outperforms many standard and benchmark activations, achieving an MSE three orders of magnitude lower than tanh on heat problems and generalizing well to finer grids and points beyond the training domain. It also excels at function approximation tasks and improves noise rejection in inverse problems, leading to more accurate parameter estimates across varying noise levels.}
}

@misc{a104fe01d341f235fd80ea98d6a8f35b8110df1d,
  author   = {Ameya Dilip Jagtap and Kenji Kawaguchi and G. Karniadakis},
  title    = {Locally adaptive activation functions with slope recovery term for deep and physics-informed neural networks},
  journal  = {ArXiv},
  url      = {https://arxiv.org/abs/1909.12228},
  doi      = {10.1098/rspa.2020.0334},
  year     = {2019},
  volume   = {abs/1909.12228},
  abstract = {We propose two approaches of locally adaptive activation functions namely, layer-wise and neuron-wise locally adaptive activation functions, which improve the performance of deep and physics-informed neural networks. The local adaptation of activation function is achieved by introducing a scalable parameter in each layer (layer-wise) and for every neuron (neuron-wise) separately, and then optimizing it using a variant of stochastic gradient descent algorithm. In order to further increase the training speed, an activation slope based slope recovery term is added in the loss function, which further accelerates convergence, thereby reducing the training cost. On the theoretical side, we prove that in the proposed method, the gradient descent algorithms are not attracted to sub-optimal critical points or local minima under practical conditions on the initialization and learning rate, and that the gradient dynamics of the proposed method is not achievable by base methods with any (adaptive) learning rates. We further show that the adaptive activation methods accelerate the convergence by implicitly multiplying conditioning matrices to the gradient of the base method without any explicit computation of the conditioning matrix and the matrix-vector product. The different adaptive activation functions are shown to induce different implicit conditioning matrices. Furthermore, the proposed methods with the slope recovery are shown to accelerate the training process.}
}

@misc{fe520ccac2a6bd50f75a4a34022fe54116871013,
  author   = {Honghui Wang and Lu Lu and Shiji Song and Gao Huang},
  title    = {Learning Specialized Activation Functions for Physics-informed Neural Networks},
  journal  = {ArXiv},
  year     = {2023},
  doi      = {10.4208/cicp.OA-2023-0058},
  url      = {https://doi.org/10.4208/cicp.OA-2023-0058},
  volume   = {abs/2308.04073},
  abstract = {Physics-informed neural networks (PINNs) are known to suffer from optimization difficulty. In this work, we reveal the connection between the optimization difficulty of PINNs and activation functions. Specifically, we show that PINNs exhibit high sensitivity to activation functions when solving PDEs with distinct properties. Existing works usually choose activation functions by inefficient trial-and-error. To avoid the inefficient manual selection and to alleviate the optimization difficulty of PINNs, we introduce adaptive activation functions to search for the optimal function when solving different problems. We compare different adaptive activation functions and discuss their limitations in the context of PINNs. Furthermore, we propose to tailor the idea of learning combinations of candidate activation functions to the PINNs optimization, which has a higher requirement for the smoothness and diversity on learned functions. This is achieved by removing activation functions which cannot provide higher-order derivatives from the candidate set and incorporating elementary functions with different properties according to our prior knowledge about the PDE at hand. We further enhance the search space with adaptive slopes. The proposed adaptive activation function can be used to solve different PDE systems in an interpretable way. Its effectiveness is demonstrated on a series of benchmarks. Code is available at https://github.com/LeapLabTHU/AdaAFforPINNs.}
}

@article{Sutfeld2018-io,
  title        = {Adaptive Blending Units: Trainable activation functions for
                  deep neural networks},
  author       = {S{\"u}tfeld, Leon Ren{\'e} and Brieger, Flemming and Finger,
                  Holger and F{\"u}llhase, Sonja and Pipa, Gordon},
  abstract     = {The most widely used activation functions in current deep
                  feed-forward neural networks are rectified linear units
                  (ReLU), and many alternatives have been successfully applied,
                  as well. However, none of the alternatives have managed to
                  consistently outperform the rest and there is no unified
                  theory connecting properties of the task and network with
                  properties of activation functions for most efficient
                  training. A possible solution is to have the network learn
                  its preferred activation functions. In this work, we
                  introduce Adaptive Blending Units (ABUs), a trainable linear
                  combination of a set of activation functions. Since ABUs
                  learn the shape, as well as the overall scaling of the
                  activation function, we also analyze the effects of adaptive
                  scaling in common activation functions. We experimentally
                  demonstrate advantages of both adaptive scaling and ABUs over
                  common activation functions across a set of systematically
                  varied network specifications. We further show that adaptive
                  scaling works by mitigating covariate shifts during training,
                  and that the observed advantages in performance of ABUs
                  likewise rely largely on the activation function's ability to
                  adapt over the course of training.},
  year         = 2018,
  primaryclass = {cs.LG},
  eprint       = {1806.10064},
  url          = {https://arxiv.org/abs/1806.10064},
  doi          = {10.48550/arXiv.1806.10064}
}

@inproceedings{miyagawa2024physicsinformed,
  title     = {Physics-informed Neural Networks for Functional Differential Equations: Cylindrical Approximation and Its Convergence Guarantees},
  author    = {Taiki Miyagawa and Takeru Yokota},
  booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year      = {2024},
  url       = {https://openreview.net/forum?id=H5z0XqEX57}
}

@article{Karniadakis2021,
  author  = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu
             and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  title   = {Physics-Informed Machine Learning},
  journal = {Nature Reviews Physics},
  volume  = {3},
  number  = {6},
  pages   = {422-440},
  year    = {2021},
  doi     = {10.1038/s42254-021-00314-5},
  url     = {https://doi.org/10.1038/s42254-021-00314-5}
}

@article{raissi2019physics,
  title    = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  journal  = {Journal of Computational Physics},
  volume   = {378},
  pages    = {686-707},
  year     = {2019},
  issn     = {0021-9991},
  doi      = {10.1016/j.jcp.2018.10.045},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
  author   = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
  keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge-Kutta methods, Nonlinear dynamics},
  abstract = {We introduce physics-informed neural networks - neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{Raissi_2018,
   title     = {Hidden physics models: Machine learning of nonlinear partial differential equations},
   volume    = {357},
   ISSN      = {0021-9991},
   url       = {http://dx.doi.org/10.1016/j.jcp.2017.11.039},
   DOI       = {10.1016/j.jcp.2017.11.039},
   journal   = {Journal of Computational Physics},
   publisher = {Elsevier BV},
   author    = {Raissi, Maziar and Karniadakis, George Em},
   year      = {2018},
   month     = mar,
   pages     = {125-141} 
}

@article{cai2021physics,
  author  = {Cai, S. and Wang, Z. and Wang, S. and Perdikaris, P. and Karniadakis, G. E.},
  title   = {Physics-informed neural networks for heat transfer problems},
  journal = {Journal of Heat Transfer},
  volume  = {143},
  number  = {6},
  pages   = {060801},
  year    = {2021},
  doi     = {10.1115/1.4050542},
  url     = {https://asmedigitalcollection.asme.org/heattransfer/article/143/6/060801/1104439/Physics-Informed-Neural-Networks-for-Heat-Transfer}
}

@article{karniadakis2021physics,
  author   = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  title    = {Physics-informed machine learning},
  journal  = {Nature Reviews Physics},
  year     = {2021},
  month    = {Jun},
  day      = {01},
  volume   = {3},
  number   = {6},
  pages    = {422-440},
  abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
  issn     = {2522-5820},
  doi      = {10.1038/s42254-021-00314-5},
  url      = {https://doi.org/10.1038/s42254-021-00314-5}
}

@book{batchelor2000introduction,
  author    = {Batchelor, G. K.},
  title     = {An Introduction to Fluid Dynamics},
  publisher = {Cambridge University Press},
  year      = {2000}
}
@inbook{batchelor2000introduction,
  place      = {Cambridge},
  series     = {Cambridge Mathematical Library},
  title      = {Kinematics of the Flow Field},
  booktitle  = {An Introduction to Fluid Dynamics},
  publisher  = {Cambridge University Press},
  author     = {Batchelor, G. K.},
  year       = {2000},
  collection = {Cambridge Mathematical Library},
  url        = {https://archive.org/details/introductiontofl0000batc}
}

@article{yang2019adversarial,
  author  = {Yang, L. and Zhang, D. and Karniadakis, G. E.},
  title   = {Physics-informed generative adversarial networks for stochastic differential equations},
  journal = {SIAM Journal on Scientific Computing},
  volume  = {41},
  number  = {1},
  pages   = {A322--A337},
  year    = {2019},
  url     = {https://arxiv.org/pdf/1811.02033}
}

@article{jin2021nsfnets,
   title     = {NSFnets (Navier-Stokes flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations},
   volume    = {426},
   ISSN      = {0021-9991},
   url       = {http://dx.doi.org/10.1016/j.jcp.2020.109951},
   DOI       = {10.1016/j.jcp.2020.109951},
   journal   = {Journal of Computational Physics},
   publisher = {Elsevier BV},
   author    = {Jin, Xiaowei and Cai, Shengze and Li, Hui and Karniadakis, George Em},
   year      = {2021},
   month     = feb,
   pages     = {109951} 
}

@misc{cuomo2022scientific,
  title         = {Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next},
  author        = {Salvatore Cuomo and Vincenzo Schiano di Cola and Fabio Giampaolo and Gianluigi Rozza and Maziar Raissi and Francesco Piccialli},
  year          = {2022},
  eprint        = {2201.05624},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2201.05624}
}

@article{mao2020physics,
  author  = {Mao, Z. and Jagtap, A. D. and Karniadakis, G. E.},
  title   = {Physics-informed neural networks for high-speed flows},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume  = {360},
  pages   = {112789},
  year    = {2020}
}

@article{jagtap2020conservative,
  author  = {Jagtap, A. D. and Kharazmi, E. and Karniadakis, G. E.},
  title   = {Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume  = {365},
  pages   = {113028},
  year    = {2020}
}

@article{Tommaso2024pinn,
  author   = {Tommaso, B. and Marco, F. and Paolo, N. and Lorenzo, P.},
  title    = {Using Physics-Informed Neural Networks for Solving Navier-Stokes Equations in Complex Scenarios},
  journal  = {SSRN},
  year     = {2024},
  pages    = {25},
  url      = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4963586},
  abstract = {Physics-Informed Neural Networks (PINNs) offer a promising approach for solving Navier-Stokes equations in fluid dynamics by embedding physical laws directly into the training process.}
}

@misc{neuralpde2023,
  doi       = {10.48550/ARXIV.2107.09443},
  url       = {https://arxiv.org/abs/2107.09443},
  author    = {Zubov, Kirill and McCarthy, Zoe and Ma, Yingbo and Calisto, Francesco and Pagliarino, Valerio and Azeglio, Simone and Bottero, Luca and Luján, Emmanuel and Sulzer, Valentin and Bharambe, Ashutosh and Vinchhi, Nand and Balakrishnan, Kaushik and Upadhyay, Devesh and Rackauckas, Chris},
  keywords  = {Mathematical Software (cs.MS), Symbolic Computation (cs.SC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {NeuralPDE: Automating Physics-Informed Neural Networks (PINNs) with Error Approximations},
  publisher = {arXiv},
  year      = {2021},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@article{Qi2024PINNs,
  author  = {Qi, Xin and others},
  title   = {Physics-informed neural networks for solving flow problems modeled by the 2D Shallow Water Equations},
  journal = {Journal of Hydrology},
  year    = {2024},
  url     = {https://eprints.soton.ac.uk/489296/2/1-s2.0-S0022169424006589-main.pdf}
}

@article{raissi2020hidden,
  author  = {Raissi, M. and Yazdani, A. and Karniadakis, G. E.},
  title   = {Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations},
  journal = {Science},
  volume  = {367},
  number  = {6481},
  pages   = {1026--1030},
  year    = {2020},
  url     = {10.1126/science.aaw4741}
}

@book{ferziger2019computational,
  author    = {Ferziger, J. H. and Perić, M. and Street, R. L.},
  title     = {Computational Methods for Fluid Dynamics},
  publisher = {Springer},
  year      = {2019}
}

@article{kochkov2021machine,
  author  = {Kochkov, D. and Smith, J. A. and Alieva, A. and Wang, Q. and Brenner, M. P. and Hoyer, S.},
  title   = {Machine learning-accelerated computational fluid dynamics},
  journal = {Proceedings of the National Academy of Sciences},
  volume  = {118},
  number  = {21},
  pages   = {e2101784118},
  year    = {2021}
}

@misc{baydin2018automatic,
  title         = {Automatic differentiation in machine learning: a survey},
  author        = {Atilim Gunes Baydin and Barak A. Pearlmutter and Alexey Andreyevich Radul and Jeffrey Mark Siskind},
  year          = {2018},
  eprint        = {1502.05767},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SC},
  url           = {https://arxiv.org/abs/1502.05767}
}

@article{zhu2019physics,
  title     = {Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data},
  volume    = {394},
  issn      = {0021-9991},
  url       = {http://dx.doi.org/10.1016/j.jcp.2019.05.024},
  doi       = {10.1016/j.jcp.2019.05.024},
  journal   = {Journal of Computational Physics},
  publisher = {Elsevier BV},
  author    = {Zhu, Yinhao and Zabaras, Nicholas and Koutsourelakis, Phaedon-Stelios and Perdikaris, Paris},
  year      = {2019},
  month     = oct,
  pages     = {56-81}
}

@article{krishnapriyan2021characterizing,
  author  = {Krishnapriyan, A. and Gholami, A. and Zhe, S. and Kirby, R. and Mahoney, M. W.},
  title   = {Characterizing possible failure modes in physics-informed neural networks},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {26548--26560},
  year    = {2021},
  url     = {https://arxiv.org/pdf/2109.01050}
}

@misc{wang2022respecting,
  title         = {Respecting causality is all you need for training physics-informed neural networks},
  author        = {Sifan Wang and Shyam Sankaran and Paris Perdikaris},
  year          = {2022},
  eprint        = {2203.07404},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2203.07404}
}

@misc{wang2021understanding,
  title         = {Understanding and mitigating gradient pathologies in physics-informed neural networks},
  author        = {Sifan Wang and Yujun Teng and Paris Perdikaris},
  year          = {2020},
  eprint        = {2001.04536},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2001.04536}
}

@article{jagtap2022physics,
  author  = {Kashinath, Karthik and Mustafa, M. and Albert, Adrian and Wu, Jinlong and Jiang, C. and Esmaeilzadeh, Soheil and Azizzadenesheli, Kamyar and Wang, R. and Chattopadhyay, Ashesh and Singh, A. and Manepalli, A. and Chirila, D. and Yu, R. and Walters, R. and White, B. and Xiao, Heng and Tchelepi, Hamdi and Marcus, P. and Anandkumar, Animashree and Prabhat, Mr},
  year    = {2021},
  month   = {02},
  pages   = {20200093},
  title   = {Physics-informed machine learning: Case studies for weather and climate modelling},
  volume  = {379},
  journal = {Philosophical transactions. Series A, Mathematical, physical, and engineering sciences},
  doi     = {10.1098/rsta.2020.0093},
  url     = {http://dx.doi.org/10.1098/rsta.2020.0093}
}

@article{sun2020surrogate,
  title     = {Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data},
  volume    = {361},
  issn      = {0045-7825},
  url       = {http://dx.doi.org/10.1016/j.cma.2019.112732},
  doi       = {10.1016/j.cma.2019.112732},
  journal   = {Computer Methods in Applied Mechanics and Engineering},
  publisher = {Elsevier BV},
  author    = {Sun, Luning and Gao, Han and Pan, Shaowu and Wang, Jian-Xun},
  year      = {2020},
  month     = apr,
  pages     = {112732}
}


@article{lu2021deepxde,
  title     = {DeepXDE: A Deep Learning Library for Solving Differential Equations},
  volume    = {63},
  issn      = {1095-7200},
  url       = {http://dx.doi.org/10.1137/19M1274067},
  doi       = {10.1137/19m1274067},
  number    = {1},
  journal   = {SIAM Review},
  publisher = {Society for Industrial & Applied Mathematics (SIAM)},
  author    = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
  year      = {2021},
  month     = jan,
  pages     = {208–228}
}