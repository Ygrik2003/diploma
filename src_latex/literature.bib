@misc{d6d98d35ebcc5cdc95330ce821728ac3d23d8292,
  author   = {O. Sallam and Mirjam Furth},
  title    = {Inference of water waves surface elevation from horizontal velocity components using physics informed neural networks (PINN)},
  journal  = {Unknown Journal},
  year     = {2024},
  abstract = {In this paper, a mathematical model is presented to infer the wave free surface elevation from the horizontal velocity components using Physics Informed Neural Network (PINN). PINN is a deep learning framework to solve forward and inverse Ordinary/Partial Differential Equations (ODEs/PDEs). The model is verified by measuring a numerically generated Kelvin waves downstream of a KRISO Container Ship (KCS). The KCS Kelvin waves are generated using two phase Volume of Fluid (VoF) Computational Fluid Dynamics (CFD) simulation with OpenFOAM. In addition, the paper presented the use of the Fourier Features decomposition of the Neural Network inputs to avoid the spectral bias phenomena; Spectral bias is the tendency of Neural Network to converge towards the low frequency solution faster than the high frequency one. Fourier Features decomposition layer showed an improvement for the model learning, as the model was able to learn the high and low frequency components simultaneously.}
}

@misc{7fcd4b3c875d8e41eb0c184aa1a42bf4c8906d61,
  author   = {Raphael Leiteritz and Marcel Hurler and D. Pflüger},
  title    = {Learning Free-Surface Flow with Physics-Informed Neural Networks},
  journal  = {2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  year     = {2021},
  doi      = {10.1109/ICMLA52953.2021.00266},
  pages    = {1668-1673},
  abstract = {The interface between data-driven learning methods and classical simulation poses an interesting field offering a multitude of new applications. In this work, we build on the notion of physics-informed neural networks (PINNs) and employ them in the area of shallow-water equation (SWE) models. These models play an important role in modeling and simulating free-surface flow scenarios such as in flood-wave propagation or tsunami waves. Different formulations of the PINN residual are compared to each other and multiple optimizations are being evaluated to speed up the convergence rate. We test these with different 1-D and 2-D experiments and finally demonstrate that regarding a SWE scenario with varying bathymetry, the method is able to produce competitive results in comparison to the direct numerical simulation with a total relative L2 error of 8.9e−3.}
}

@misc{0fc26db616856a9dea2be1597b21842d80c45f54,
  author   = {Rui Gao and R. Jaiman},
  title    = {H-SIREN: Improving implicit neural representations with hyperbolic periodic functions},
  journal  = {ArXiv},
  year     = {2024},
  doi      = {10.48550/arXiv.2410.04716},
  volume   = {abs/2410.04716},
  abstract = {Implicit neural representations (INR) have been recently adopted in various applications ranging from computer vision tasks to physics simulations by solving partial differential equations. Among existing INR-based works, multi-layer perceptrons with sinusoidal activation functions find widespread applications and are also frequently treated as a baseline for the development of better activation functions for INR applications. Recent investigations claim that the use of sinusoidal activation functions could be sub-optimal due to their limited supported frequency set as well as their tendency to generate over-smoothed solutions. We provide a simple solution to mitigate such an issue by changing the activation function at the first layer from \$\\sin(x)\$ to \$\\sin(\\sinh(2x))\$. We demonstrate H-SIREN in various computer vision and fluid flow problems, where it surpasses the performance of several state-of-the-art INRs.}
}

@misc{0d752c79fb816703274a3d37f85a85689a2a9405,
  author   = {Sourav Mishra and Shreya Hallikeri and Suresh Sundaram},
  title    = {REAct: Rational Exponential Activation for Better Learning and Generalization in PINNs},
  journal  = {Unknown Journal},
  year     = {2025},
  doi      = {10.1109/ICASSP49660.2025.10887788},
  abstract = {Physics-Informed Neural Networks (PINNs) offer a promising approach to simulating physical systems. Still, their application is limited by optimization challenges, mainly due to the lack of activation functions that generalize well across several physical systems. Existing activation functions often lack such flexibility and generalization power. To address this issue, we introduce Rational Exponential Activation (REAct), a generalized form of tanh consisting of four learnable shape parameters. Experiments show that REAct outperforms many standard and benchmark activations, achieving an MSE three orders of magnitude lower than tanh on heat problems and generalizing well to finer grids and points beyond the training domain. It also excels at function approximation tasks and improves noise rejection in inverse problems, leading to more accurate parameter estimates across varying noise levels.}
}

@misc{a104fe01d341f235fd80ea98d6a8f35b8110df1d,
  author   = {Ameya Dilip Jagtap and Kenji Kawaguchi and G. Karniadakis},
  title    = {Locally adaptive activation functions with slope recovery term for deep and physics-informed neural networks},
  journal  = {ArXiv},
  year     = {2019},
  volume   = {abs/1909.12228},
  abstract = {We propose two approaches of locally adaptive activation functions namely, layer-wise and neuron-wise locally adaptive activation functions, which improve the performance of deep and physics-informed neural networks. The local adaptation of activation function is achieved by introducing a scalable parameter in each layer (layer-wise) and for every neuron (neuron-wise) separately, and then optimizing it using a variant of stochastic gradient descent algorithm. In order to further increase the training speed, an activation slope based slope recovery term is added in the loss function, which further accelerates convergence, thereby reducing the training cost. On the theoretical side, we prove that in the proposed method, the gradient descent algorithms are not attracted to sub-optimal critical points or local minima under practical conditions on the initialization and learning rate, and that the gradient dynamics of the proposed method is not achievable by base methods with any (adaptive) learning rates. We further show that the adaptive activation methods accelerate the convergence by implicitly multiplying conditioning matrices to the gradient of the base method without any explicit computation of the conditioning matrix and the matrix-vector product. The different adaptive activation functions are shown to induce different implicit conditioning matrices. Furthermore, the proposed methods with the slope recovery are shown to accelerate the training process.}
}

@misc{fe520ccac2a6bd50f75a4a34022fe54116871013,
  author   = {Honghui Wang and Lu Lu and Shiji Song and Gao Huang},
  title    = {Learning Specialized Activation Functions for Physics-informed Neural Networks},
  journal  = {ArXiv},
  year     = {2023},
  doi      = {10.4208/cicp.OA-2023-0058},
  volume   = {abs/2308.04073},
  abstract = {Physics-informed neural networks (PINNs) are known to suffer from optimization difficulty. In this work, we reveal the connection between the optimization difficulty of PINNs and activation functions. Specifically, we show that PINNs exhibit high sensitivity to activation functions when solving PDEs with distinct properties. Existing works usually choose activation functions by inefficient trial-and-error. To avoid the inefficient manual selection and to alleviate the optimization difficulty of PINNs, we introduce adaptive activation functions to search for the optimal function when solving different problems. We compare different adaptive activation functions and discuss their limitations in the context of PINNs. Furthermore, we propose to tailor the idea of learning combinations of candidate activation functions to the PINNs optimization, which has a higher requirement for the smoothness and diversity on learned functions. This is achieved by removing activation functions which cannot provide higher-order derivatives from the candidate set and incorporating elementary functions with different properties according to our prior knowledge about the PDE at hand. We further enhance the search space with adaptive slopes. The proposed adaptive activation function can be used to solve different PDE systems in an interpretable way. Its effectiveness is demonstrated on a series of benchmarks. Code is available at https://github.com/LeapLabTHU/AdaAFforPINNs.}
}

@article{Sutfeld2018-io,
  title        = {Adaptive Blending Units: Trainable activation functions for
                  deep neural networks},
  author       = {S{\"u}tfeld, Leon Ren{\'e} and Brieger, Flemming and Finger,
                  Holger and F{\"u}llhase, Sonja and Pipa, Gordon},
  abstract     = {The most widely used activation functions in current deep
                  feed-forward neural networks are rectified linear units
                  (ReLU), and many alternatives have been successfully applied,
                  as well. However, none of the alternatives have managed to
                  consistently outperform the rest and there is no unified
                  theory connecting properties of the task and network with
                  properties of activation functions for most efficient
                  training. A possible solution is to have the network learn
                  its preferred activation functions. In this work, we
                  introduce Adaptive Blending Units (ABUs), a trainable linear
                  combination of a set of activation functions. Since ABUs
                  learn the shape, as well as the overall scaling of the
                  activation function, we also analyze the effects of adaptive
                  scaling in common activation functions. We experimentally
                  demonstrate advantages of both adaptive scaling and ABUs over
                  common activation functions across a set of systematically
                  varied network specifications. We further show that adaptive
                  scaling works by mitigating covariate shifts during training,
                  and that the observed advantages in performance of ABUs
                  likewise rely largely on the activation function's ability to
                  adapt over the course of training.},
  year         = 2018,
  primaryclass = {cs.LG},
  eprint       = {1806.10064}
}

@inproceedings{miyagawa2024physicsinformed,
  title     = {Physics-informed Neural Networks for Functional Differential Equations: Cylindrical Approximation and Its Convergence Guarantees},
  author    = {Taiki Miyagawa and Takeru Yokota},
  booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year      = {2024},
  url       = {https://openreview.net/forum?id=H5z0XqEX57}
}

@article{Karniadakis2021,
  author  = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu
             and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  title   = {Physics-Informed Machine Learning},
  journal = {Nature Reviews Physics},
  volume  = {3},
  number  = {6},
  pages   = {422-440},
  year    = {2021},
  url     = {https://doi.org/10.1038/s42254-021-00314-5}
}

@article{raissi2019physics,
  author  = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
  title   = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  journal = {Journal of Computational Physics},
  volume  = {378},
  pages   = {686--707},
  year    = {2019}
}

@article{raissi2017physics,
  author  = {Raissi, M. and Karniadakis, G. E.},
  title   = {Hidden physics models: Machine learning of nonlinear partial differential equations},
  journal = {Journal of Computational Physics},
  volume  = {357},
  pages   = {125--141},
  year    = {2017}
}

@article{cai2021physics,
  author  = {Cai, S. and Wang, Z. and Wang, S. and Perdikaris, P. and Karniadakis, G. E.},
  title   = {Physics-informed neural networks for heat transfer problems},
  journal = {Journal of Heat Transfer},
  volume  = {143},
  number  = {6},
  pages   = {060801},
  year    = {2021}
}

@article{karniadakis2021physics,
  author  = {Karniadakis, G. E. and Kevrekidis, I. G. and Lu, L. and Perdikaris, P. and Wang, S. and Yang, L.},
  title   = {Physics-informed machine learning},
  journal = {Nature Reviews Physics},
  volume  = {3},
  number  = {6},
  pages   = {422--440},
  year    = {2021}
}

@book{batchelor2000introduction,
  author    = {Batchelor, G. K.},
  title     = {An Introduction to Fluid Dynamics},
  publisher = {Cambridge University Press},
  year      = {2000}
}

@article{yang2019adversarial,
  author  = {Yang, L. and Zhang, D. and Karniadakis, G. E.},
  title   = {Physics-informed generative adversarial networks for stochastic differential equations},
  journal = {SIAM Journal on Scientific Computing},
  volume  = {41},
  number  = {1},
  pages   = {A322--A337},
  year    = {2019}
}

@article{jin2021nsfnets,
  author  = {Jin, X. and Cai, S. and Li, H. and Karniadakis, G. E.},
  title   = {NSFnets (Navier-Stokes flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations},
  journal = {Journal of Computational Physics},
  volume  = {426},
  pages   = {109951},
  year    = {2021}
}

@article{cuomo2022scientific,
  author  = {Cuomo, S. and Di Cola, V. S. and Giampaolo, F. and Rozza, G. and Raissi, M. and Piccialli, F.},
  title   = {Scientific machine learning through physics-informed neural networks: Where we are and what's next},
  journal = {Journal of Scientific Computing},
  volume  = {92},
  number  = {3},
  pages   = {88},
  year    = {2022}
}

@article{mao2020physics,
  author  = {Mao, Z. and Jagtap, A. D. and Karniadakis, G. E.},
  title   = {Physics-informed neural networks for high-speed flows},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume  = {360},
  pages   = {112789},
  year    = {2020}
}

@article{jagtap2020conservative,
  author  = {Jagtap, A. D. and Kharazmi, E. and Karniadakis, G. E.},
  title   = {Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume  = {365},
  pages   = {113028},
  year    = {2020}
}

@article{omarova2022pinn,
  author  = {Омарова, П.},
  title   = {Применение физически информированных нейронных сетей для моделирования Навье-Стокса},
  journal = {Вестник компьютерных и информационных технологий},
  volume  = {7},
  number  = {15},
  pages   = {45--59},
  year    = {2022}
}

@misc{neuralpde2023,
  author = {NeuralPDE.jl},
  title  = {A physics-informed neural network solver for Julia},
  year   = {2023},
  note   = {GitHub Repository}
}

@article{fang2021high,
  author  = {Fang, J. and Gao, H. and Sun, J.},
  title   = {High-fidelity flow simulation based on physics-informed deep learning},
  journal = {Journal of Fluid Mechanics},
  volume  = {926},
  pages   = {A10},
  year    = {2021}
}

@article{raissi2020hidden,
  author  = {Raissi, M. and Yazdani, A. and Karniadakis, G. E.},
  title   = {Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations},
  journal = {Science},
  volume  = {367},
  number  = {6481},
  pages   = {1026--1030},
  year    = {2020}
}

@article{omarova2022syrdarya,
  author  = {Омарова, П.},
  title   = {Прогнозирование заиливания речных русел с использованием физически информированных нейронных сетей на примере реки Сырдарья},
  journal = {Гидротехническое строительство},
  volume  = {5},
  number  = {23},
  pages   = {112--128},
  year    = {2022}
}

@article{blumenau2021reconstruction,
  author  = {Блуменау, М. И.},
  title   = {Реконструкция течений на основе разреженных данных методом физически информированных нейронных сетей},
  journal = {Физика жидкости и газа},
  volume  = {8},
  number  = {12},
  pages   = {34--48},
  year    = {2021}
}

@book{ferziger2019computational,
  author    = {Ferziger, J. H. and Perić, M. and Street, R. L.},
  title     = {Computational Methods for Fluid Dynamics},
  publisher = {Springer},
  year      = {2019}
}

@article{kochkov2021machine,
  author  = {Kochkov, D. and Smith, J. A. and Alieva, A. and Wang, Q. and Brenner, M. P. and Hoyer, S.},
  title   = {Machine learning–accelerated computational fluid dynamics},
  journal = {Proceedings of the National Academy of Sciences},
  volume  = {118},
  number  = {21},
  pages   = {e2101784118},
  year    = {2021}
}

@article{baydin2018automatic,
  author  = {Baydin, A. G. and Pearlmutter, B. A. and Radul, A. A. and Siskind, J. M.},
  title   = {Automatic differentiation in machine learning: A survey},
  journal = {Journal of Machine Learning Research},
  volume  = {18},
  pages   = {1--43},
  year    = {2018}
}

@article{zhu2019physics,
  author  = {Zhu, Y. and Zabaras, N. and Koutsourelakis, P. S. and Perdikaris, P.},
  title   = {Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data},
  journal = {Journal of Computational Physics},
  volume  = {394},
  pages   = {56--81},
  year    = {2019}
}

@article{krishnapriyan2021characterizing,
  author  = {Krishnapriyan, A. and Gholami, A. and Zhe, S. and Kirby, R. and Mahoney, M. W.},
  title   = {Characterizing possible failure modes in physics-informed neural networks},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {26548--26560},
  year    = {2021}
}

@article{wang2022respecting,
  author  = {Wang, S. and Yu, X. and Perdikaris, P.},
  title   = {Respecting causality is all you need for training physics-informed neural networks},
  journal = {arXiv preprint},
  year    = {2022},
  eprint  = {2203.07404}
}

@article{wang2021understanding,
  author  = {Wang, S. and Teng, Y. and Perdikaris, P.},
  title   = {Understanding and mitigating gradient flow pathologies in physics-informed neural networks},
  journal = {SIAM Journal on Scientific Computing},
  volume  = {43},
  number  = {5},
  pages   = {A3055--A3081},
  year    = {2021}
}

@article{jagtap2022physics,
  author  = {Jagtap, A. D. and Kawaguchi, K. and Karniadakis, G. E.},
  title   = {Physics-informed machine learning: Case studies for weather and climate modelling},
  journal = {Philosophical Transactions of the Royal Society A},
  volume  = {380},
  number  = {2229},
  pages   = {20210287},
  year    = {2022}
}

@article{sun2020surrogate,
  author  = {Sun, L. and Gao, H. and Pan, S. and Wang, J. X.},
  title   = {Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume  = {361},
  pages   = {112732},
  year    = {2020}
}

@article{lu2021deepxde,
  author  = {Lu, L. and Meng, X. and Mao, Z. and Karniadakis, G. E.},
  title   = {DeepXDE: A deep learning library for solving differential equations},
  journal = {SIAM Review},
  volume  = {63},
  number  = {1},
  pages   = {208--228},
  year    = {2021}
}