\chapter{Результаты}
Для получения более качественных результатов, а также многократоно
сократив время расчетов и исследований, проведем кроссвалидацию
гиперпараметров модели в два этапа. Сперва используем адаптивную
функцию активации REAct \cite{0d752c79fb816703274a3d37f85a85689a2a9405},
в силу ее простоты и скорости вычисления значения и производной, что 
позволяет проводить кроссвалидацию намного эффективнее. Так как теоритически
нельзя предсказать поведение модели при разных параметрах, кроссвалидация
проводится по средствам поиска по сетке. В качестве
гиперпараметров будем итерировать конфигурацию нейронной сети, оптимизатору,
количеству точек внутри области и скорости обучения.

Архитектура нейронной сети исследовалась с вариативным подбором слоёв:
от компактных структур, таких как $16-16$ и $32-32$, до более глубоких
комбинаций --- $32-64-32$, $64-32-64$ и даже асимметричных схем, включающих
$16-32-64$, $64-32-16$, $16-64-32$ и $64-16-32$. Отдельное внимание уделялось
масштабируемым конфигурациям, таким как $64-64$, для оценки влияния
количества нейронов на производительность модели.

Для исследования влияния оптимизатора на результат были выбраны
следующие оптимизаторы: \textbf{Adam}, \textbf{Adagrad}, \textbf{Adamax},
а также специализированные \textbf{ASGD} и \textbf{RMSprop}.

Обучающая выборка варьировалась между $100$ и $500$ точками для внутренней
области и неизменных $1000$ точек для граничных условий. 

Скорость обучения задавалась тремя значениями: $10^{-1}$, $10^{-2}$ и $10^{-3}$.


\begin{figure}[ht]
    \includegraphics{data/couette_react_error_best.png}
    \caption{Лучший результат в процессе кроссвалидации с функцией активации REAct}
    \label{fig:couette_react_best}
\end{figure}


В результате обучения модель с предсказанным минимальным отклонением от
верного решения (рис. \ref{fig:couette_react_best}) имела следующие значения
гиперпараметров (табл. \ref{table:couette_react_best_params}).

\begin{table}[h!]
    \centering
    \begin{tabular}{ |c|c| } 
        \hline
        Функция активации & $\text{REAct}(0.8, -4, -3.2, 0.2)$ \\
        \hline
        Оптимизатор & Adagrad \\ 
        \hline
        Конфигурация сети & $64-16-32$ \\ 
        \hline
        Скорость обучения & $10^{-3}$ \\ 
        \hline
        Количество точек внутри области & $100$ \\ 
        \hline
    \end{tabular}
    \caption{Значения гиперпараметров у модели с лучшим результатом}
    \label{table:couette_react_best_params}
\end{table}

В результате анализа остальных моделей было посчитано количество нулевых
решений, решений с максимальным отклонением до 20\%, а также больше 20\% и 50\%.
Помимо перечисленных также было несколько моделей, ушедших в процессе обучения в
NaN (рис. \ref{fig:couette_react_stat}). 

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \pgfplotstableread{
        Label   First
        NaN                     4
        Нулевое\ решение        30
        $>50\%$                 16
        $>20\%$                 94
        $<20\%$                 17
        }\datatable
        
        \begin{axis}[
            xbar stacked,
            xmin=0,
            ytick=data,
            yticklabels from table={\datatable}{Label}
            ]
            \addplot [fill=yellow] table [x=First, y expr=\coordindex] {\datatable};
        \end{axis}
    \end{tikzpicture}
    \caption{Распределение результатов кроссвалидации с функцией активации REAct}
    \label{fig:couette_react_stat}
\end{figure}

Также стоит отметить, что все нулевые решения соответствовали функции активации,
которая на всей области определения имеет отрицательные значения.

В качестве корректных параметров были отобраны те, которые соответствуют критерию
принадлежности к группе с уровнем менее 20\%. В результате, во второй этап были
включены следующие параметры:
\begin{enumerate}
    \item Конфигурации с резким переходом между слоями ($16-64-32$ и $64-16-32$) а также
    с линейным переходом ($16-32-64$ и $64-32-16$) не показали особых результатов по
    сравнению с остальными трехслойными сетями. Аналогично конфигурации
    $16-16$ и $32-32$ показали себя хуже по сравнению с $64-64$.
    \item По количеству точек внутри области значительно больше удачных
    результатов было при $100$ точек. Это поведение характеризуется 
    соотношением точек на границе и внутри области. Данное соотношение 
    показывает значимость граничных условий, что необходимо для избегания
    нулевого решения
    \item Скорость обучения влияет на то, сможет ли оптимизатор 
    выбраться из локального минимума. Лучший результат показало значение
    скорости обучения равное $10^{-3}$
    \item Среди оптимизаторов лучшие результаты показали \textbf{Adam}, \textbf{Adagrad} и
    \textbf{ASGD}, в свою очередь \textbf{Adamax} и \textbf{RSMProp} являются узкоспециализированными,
    что требует точной настройки параметров, что в данной работе не рассматривается.
\end{enumerate} 

Теперь получив более точное представление о наших параметрах можно приступить
ко второму этапу обучения с использованием комплексной функцией активации ABU \cite{Sutfeld2018-io}.

Для более корректной оценки результатов будем анализировать медиану для каждого параметра
на протяжении всего этапа обучения.
\input{images/tikz/results/loss_couette_abu_neurons.tex}
% \input{images/tikz/results/loss_couette_abu_optimizer.tex}
% \input{images/tikz/results/loss_couette_abu_scale_quadratic.tex}
% \input{images/tikz/results/loss_couette_abu_scale_softplus.tex}
% \input{images/tikz/results/loss_couette_abu_scale_swish.tex}
% \input{images/tikz/results/loss_couette_abu_scale_tanh.tex}