\chapter{Методы}
С точки зрения машинного обучения и нейронных сетей есть множество важных аспектов,
помимо точности. В контексте физически-информированных нейронных сетей речь пойдет
об оптимизации архитектуры нейронной сети для ускорения обучения, увеличения точности
для оценки компонент скоростей или давления \cite{Tommaso2024pinn}.

% MFN-PINN,  MLP-PINN
В первую очередь нужно обратить внимание на функции активации, которые определяют,
как нейрон будет реагировать на входные данные, и могут существенно влиять на способность
сети обучаться сложным функциям и обобщать полученные знания.

Каждый нейрон в нейронной сети использует функцию активации для преобразования взвешенной
суммы своих входных сигналов в выходной сигнал. Эта функция вносит нелинейность в модель,
что необходимо для обучения сложных зависимостей.

Формула, описывающая этот процесс, выглядит следующим образом:

$$y = f(\sum_{i=1}^{n} w_i x_i + b)$$

где $x_i$ — $i$-й входной сигнал нейрона, $w_i$ — вес, связанный с $i$-м входным сигналом, $b$
— смещение нейрона, $f(\cdot)$ — функция активации, $y$ — выходной сигнал нейрона.

Для эффективного использования в PINNs функции активации должны удовлетворять следующим
критериям \cite{0d752c79fb816703274a3d37f85a85689a2a9405}:
\begin{itemize}
    \item функции активации должны быть гладкими и
    непрерывно дифференцируемыми, чтобы обрабатывать функции потерь, которые включают
    производные высших порядков.
    \item функции активации должны допускать неограниченные
    выходные значения, в отличие от функций $tanh$ и $sin$, которые ограничены между $-1$ и $1$.
    \item функции активации должны избегать насыщения, чтобы предотвратить
    исчезновение градиентов, что может затруднить обучение.
    \item в некоторых случаях желательно иметь контролируемое насыщение за пределами определённого
    диапазона, чтобы улучшить способность модели представлять сложные физические сигналы.
\end{itemize}


\subsection{ReLU}
\include{images/tikz/act_func_graph.tex}
\include{images/tikz/nn_structure.tex}
ReLU (Rectified Linear Unit) — это функция активации, которая возвращает входное значение,
если оно положительное, и 0, если оно отрицательное \eqref{relu}. Это одна из самых популярных 
функций активации в нейронных сетях, благодаря своей простоте и эффективности. 
\begin{equation}
    \text{ReLU}(x) = \max(0, x)
    \label{relu}
\end{equation}
В контексте физически информированных нейронных сетей данная функция не эффективна в силу
разрава при $x = 0$ и производной, равной $1$ при положительных значениях $x$, что создает
проблемы при обратном распространении ошибки.

\subsection{SiLU}
SiLU (Sigmoid Linear Unit), также известная как $\text{Swish-}1$ \eqref{silu}, является
гладкой немонотонной функцией активации, используемой в нейронных сетях.
\begin{equation}
    \text{SiLU}(x) = x \cdot \sigma(x) \label{silu}
\end{equation}
Эта функция удобна своей гладкостью и способностью улучшать производительность глубоких
моделей машинного обучения по сравнению с функциями активации, такими как ReLU.
\subsection{Swish}
Swish является гладкой и немонотонной функцией, аналогично SiLU, но имеет дополнительный
параметр $\beta$, который может быть настроен или зафиксирован \eqref{swish}.
\begin{equation}
    \label{swish}
    swish_\beta(x) = \frac{x}{1 + e^{-\beta x}}
\end{equation}
В целом Swish является заменой всех предыдущих функций:
\begin{itemize}
    \item При $\text{swish}_1(x) = \text{SiLU}(x)$
    \item При $\text{swish}\inf(x) = \text{ReLU}(x)$
\end{itemize}

GELU (Gaussian Error Linear Unit) и
SILU (Sigmoid Linear Unit), а также стандартные ReLU (Rectified Linear Unit), Sigmoid
и Tanh. Помимо обычных функций активаций есть адаптивные, которые подразумевают подбор
специфичных коэфициентов в процессе обучения нейронной сети для улучшения "понимания"
физической составляющей искомой задачи. К таким функциям относятся функции ABU
(Adaptive Blending Unit) и REAct (Rational Exponential Activation)

