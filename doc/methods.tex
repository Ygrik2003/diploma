\chapter{Методы}
С точки зрения машинного обучения и нейронных сетей есть множество важных аспектов,
помимо точности. В контексте физически-информированных нейронных сетей речь пойдет
об оптимизации архитектуры нейронной сети для ускорения обучения, увеличения точности
для оценки компонент скоростей или давления \cite{Tommaso2024pinn}.

Нейронные сети весьма непредсказуемы при обучении, особенно в случае отсутствия
обучающих и тестовых данных. Для понимания работы нейронных сетей в таком режиме
следует изучить влияние количества нейронов, активационной функции, количества слоев,
Dropout'ов, а также обратить внимание на функции активации, которые определяют,
как нейрон будет реагировать на входные данные, что может существенно влиять на способность
сети обучаться сложным функциям и обобщать полученные данные.
\section{Постановка задачи}
В основе исследуемых задач будем использовать уравнения Навье-Стокса по следующим
причинам:
\begin{enumerate}
    \item Данные уравнения используют частные производные первого и второго порядка
    \item Подразумевается система трех уравнений для двумерной задачи
    \item Наличие двух входных и трех выходных переменных для двумерной задачи
\end{enumerate}
Такой подход позволит рассмотреть физически-информированные нейронные сети на сложных,
с точки зрения модели, задачах, тем самым изучив поведение модели в нетривиальных случаях.

Для анализа будут рассмотрены течение Куэтта и течение жидкости в канале
\subsection{Течение Куэтта}
\input{images/tikz/couette_flow.tex}
\input{images/tikz/points.tex}
Течение Куэтта представляет из себя двумерный плоский канал шириной $l$, одна из стенок которого 
движется со скоростью $u_0$. Данная задача имеет аналитическое решение:
\begin{equation}
    \begin{cases}
        u = u_0 * y / l \\
        v = 0
    \end{cases}
\end{equation}
где $y$ --- расстояние от неподвижной стенки.
Используя данное аналитическое решение можно валидировать результаты нейронной сети и
исследовать влияение наличия правильного решения в процессе обучения нейронной сети на
резульаты.
\subsection{Течение жидкости в канале}
Для усложнения задачи возьмем канал, отличный от плоского. \textcolor{red}{На занятиях
по дисциплине "Компьютерное моделирование прикладных физических задач" мы решали подобную
задачу посредством традиционных численных методов???}. Канал представляет из себя (рисунок).
% \input{images/tikz/channel_flow.tex}
Возьмем ранее упомянутое численное решение для оценки качества результата нейронной сети и
последующего внедрения данных точного решения в процесс обучения.


% MFN-PINN,  MLP-PINN

Каждый нейрон в нейронной сети использует функцию активации для преобразования взвешенной
суммы своих входных сигналов в выходной сигнал. Эта функция вносит нелинейность в модель,
что необходимо для обучения сложных зависимостей.

Формула, описывающая этот процесс, выглядит следующим образом:

$$y = f(\sum_{i=1}^{n} w_i x_i + b)$$

где $x_i$ — $i$-й входной сигнал нейрона, $w_i$ — вес, связанный с $i$-м входным сигналом, $b$
— смещение нейрона, $f(\cdot)$ — функция активации, $y$ — выходной сигнал нейрона.

Для эффективного использования в PINNs функции активации должны удовлетворять следующим
критериям \cite{0d752c79fb816703274a3d37f85a85689a2a9405}:
\begin{itemize}
    \item функции активации должны быть гладкими и
    непрерывно дифференцируемыми, чтобы обрабатывать функции потерь, которые включают
    производные высших порядков.
    \item функции активации должны допускать неограниченные
    выходные значения, в отличие от функций $tanh$ и $sin$, которые ограничены между $-1$ и $1$.
    \item функции активации должны избегать насыщения, чтобы предотвратить
    исчезновение градиентов, что может затруднить обучение.
    \item в некоторых случаях желательно иметь контролируемое насыщение за пределами определённого
    диапазона, чтобы улучшить способность модели представлять сложные физические сигналы.
\end{itemize}


\subsection{ReLU}
\input{images/tikz/act_func_graph.tex}
\input{images/tikz/react_func_graph.tex}
\input{images/tikz/abu_func_graph.tex}
\input{images/tikz/nn_structure.tex}
ReLU (Rectified Linear Unit) — это функция активации, которая возвращает входное значение,
если оно положительное, и 0, если оно отрицательное \eqref{relu}. Это одна из самых популярных 
функций активации в нейронных сетях, благодаря своей простоте и эффективности. 
\begin{equation}
    \text{ReLU}(x) = \max(0, x)
    \label{relu}
\end{equation}
В контексте физически информированных нейронных сетей данная функция не эффективна в силу
разрава при $x = 0$ и производной, равной $1$ при положительных значениях $x$, что создает
проблемы при обратном распространении ошибки.

\subsection{SiLU}
SiLU (Sigmoid Linear Unit), также известная как $\text{Swish-}1$ \eqref{silu}, является
гладкой немонотонной функцией активации, используемой в нейронных сетях.
\begin{equation}
    \text{SiLU}(x) = x \cdot \sigma(x) \label{silu}
\end{equation}
Эта функция удобна своей гладкостью и способностью улучшать производительность глубоких
моделей машинного обучения по сравнению с функциями активации, такими как ReLU.
\subsection{Swish}
Swish является гладкой и немонотонной функцией, аналогично SiLU, но имеет дополнительный
параметр $\beta$, который может быть настроен или зафиксирован \eqref{swish}.
\begin{equation}
    \label{swish}
    swish_\beta(x) = \frac{x}{1 + e^{-\beta x}}
\end{equation}
В целом Swish является заменой всех предыдущих функций:
\begin{itemize}
    \item При $\text{swish}_1(x) = \text{SiLU}(x)$
    \item При $\text{swish}\inf(x) = \text{ReLU}(x)$
\end{itemize}

GELU (Gaussian Error Linear Unit) и
SILU (Sigmoid Linear Unit), а также стандартные ReLU (Rectified Linear Unit), Sigmoid
и Tanh. Помимо обычных функций активаций есть адаптивные, которые подразумевают подбор
специфичных коэфициентов в процессе обучения нейронной сети для улучшения "понимания"
физической составляющей искомой задачи. К таким функциям относятся функции ABU
(Adaptive Blending Unit) и REAct (Rational Exponential Activation)

