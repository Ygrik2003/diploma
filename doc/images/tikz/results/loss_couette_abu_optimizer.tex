\section{Зависимость от оптимизатора}

С точки зрения оптимизатора никаких ограничений на поведение
модели нет. От выбора оптимизатора зависит на сколько сложно
модели будет выбраться из локального минимума и приблизиться
к глобальному.

Рассмотрим верхнюю границу (рис \ref{fig:bc_top_optimizer}).
Исходя из графика, при использовании оптимизатора ASGD
функция потерь в среднем стремится к $~0.25$. При такой 
функции потерь максимальное отклонение от точного решения
может достигать $50\%$, если не учитывать полностью
нулевые решения. Сам по себе ASGD является мощным оптимизатором,
но требует тонкой настройки своих параметров, поэтому он
показывает худший результат в данной задаче.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \begin{tikzpicture}[scale=0.85]
            \begin{axis}[
                ymode=log,
                legend style={font=\tiny},
                xmin=0,
                xtick distance=4000,
                axis lines=left,
                grid=both
            ]            
                \addplot+[mark=*, mark size=1pt, thick, red] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/bc_bottom_optimizer_1.csv};
                \addlegendentry{Adam}
                \addplot+[mark=*, mark size=1pt, thick, green] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/bc_bottom_optimizer_2.csv};
                \addlegendentry{Adagrad}
                \addplot+[mark=*, mark size=1pt, thick, blue] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/bc_bottom_optimizer_4.csv};
                \addlegendentry{ASGD}
            \end{axis}
        \end{tikzpicture}
        \caption{Нижняя граница}
        \label{fig:bc_bottom_optimizer}
    \end{subfigure}
    \hspace{0.5cm}
    \begin{subfigure}[b]{0.4\textwidth}
        \begin{tikzpicture}[scale=0.85]
            \begin{axis}[
                ymode=log,
                legend style={font=\tiny},
                xmin=0,
                xtick distance=4000,
                axis lines=left,
                grid=both
            ]
                \addplot+[mark=*, mark size=1pt, thick, red] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/bc_top_optimizer_1.csv};
                \addlegendentry{Adam}
                \addplot+[mark=*, mark size=1pt, thick, green] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/bc_top_optimizer_2.csv};
                \addlegendentry{Adagrad}
                \addplot+[mark=*, mark size=1pt, thick, blue] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/bc_top_optimizer_4.csv};
                \addlegendentry{ASGD}
            \end{axis}
        \end{tikzpicture}
        \caption{Верхняя граница}
        \label{fig:bc_top_optimizer}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \begin{tikzpicture}[scale=0.85]
            \begin{axis}[
                ymode=log,
                legend style={font=\tiny},
                xmin=0,
                xtick distance=4000,
                axis lines=left,
                grid=both
            ]
                \addplot+[mark=*, mark size=1pt, thick, red] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/bc_left_optimizer_1.csv};
                \addlegendentry{Adam}
                \addplot+[mark=*, mark size=1pt, thick, green] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/bc_left_optimizer_2.csv};
                \addlegendentry{Adagrad}
                \addplot+[mark=*, mark size=1pt, thick, blue] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/bc_left_optimizer_4.csv};
                \addlegendentry{ASGD}
            \end{axis}
        \end{tikzpicture}
        \caption{Левая граница}
        \label{fig:bc_left_optimizer}
    \end{subfigure}
    \hspace{0.5cm}
    \begin{subfigure}[b]{0.4\textwidth}
        \begin{tikzpicture}[scale=0.85]
            \begin{axis}[
                ymode=log,
                legend style={font=\tiny},
                xmin=0,
                xtick distance=4000,
                axis lines=left,
                grid=both
            ]
                \addplot+[mark=*, mark size=1pt, thick, red] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/bc_right_optimizer_1.csv};
                \addlegendentry{Adam}
                \addplot+[mark=*, mark size=1pt, thick, green] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/bc_right_optimizer_2.csv};
                \addlegendentry{Adagrad}
                \addplot+[mark=*, mark size=1pt, thick, blue] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/bc_right_optimizer_4.csv};
                \addlegendentry{ASGD}
            \end{axis}
        \end{tikzpicture}
        \caption{Правая граница}
        \label{fig:bc_right_optimizer}
    \end{subfigure}
    \caption{Зависимость медианы функции потерь на каждой эпохе при разных оптимизаторах}
    \label{fig:bc_loss_optimizer}
\end{figure}

Похожая ситуация с Adagrad оптимизатором, без качественной
настройки параметров скорость обучения адаптивно уменьшается
и оптимизатор застывает в локальном минимуме. Вероятнее всего
при большем числе эпох Adagrad сможет догнать Adam, но нас
данный вариант не устраивает.

Оптимизатор Adam показывает лучший результат. Данный оптимизатор
включает в себя преймущества двух предыдущих и является универсальным,
поэтому не требует такой же точной настройки параметров.

Аналогичное поведение оптимизаторов можно заметить на графике для нижней
границы (рис. \ref{fig:bc_bottom_optimizer}). На графиках левой
(рис. \ref{fig:bc_left_optimizer}) и правой (рис. \ref{fig:bc_right_optimizer})
границах можно заметить смещение оптимизатора ASGD ближе к Adagrad, что может
свидетельствовать о преобладающем нулевом решении.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.85]
            \begin{axis}[
                ymode=log,
                ymax=1e-2,
                legend style={font=\tiny},
                xmin=0,
                xtick distance=4000,
                axis lines=left,
                grid=both
            ]
                \addplot+[mark=*, mark size=1pt, thick, red] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/pde_momentum_x_optimizer_1.csv};
                \addlegendentry{Adam}
                \addplot+[mark=*, mark size=1pt, thick, green] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/pde_momentum_x_optimizer_2.csv};
                \addlegendentry{Adagrad}
                \addplot+[mark=*, mark size=1pt, thick, blue] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/pde_momentum_x_optimizer_4.csv};
                \addlegendentry{ASGD}
            \end{axis}
        \end{tikzpicture}
        \caption{Уравнение для $u_x$}
        \label{fig:pde_ux_optimizer}
    \end{subfigure}
    \hspace{0.5cm}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.85]
            \begin{axis}[
                ymode=log,
                ymax=1e-2,
                legend style={font=\tiny},
                xmin=0,
                xtick distance=4000,
                axis lines=left,
                grid=both
            ]
                \addplot+[mark=*, mark size=1pt, thick, red] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/pde_momentum_y_optimizer_1.csv};
                \addlegendentry{Adam}
                \addplot+[mark=*, mark size=1pt, thick, green] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/pde_momentum_y_optimizer_2.csv};
                \addlegendentry{Adagrad}
                \addplot+[mark=*, mark size=1pt, thick, blue] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/pde_momentum_y_optimizer_4.csv};
                \addlegendentry{ASGD}
            \end{axis}
        \end{tikzpicture}
        \caption{Уравнение для $u_y$}
        \label{fig:pde_uy_optimizer}
    \end{subfigure}
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.85]
            \begin{axis}[
                ymode=log,
                ymax=1e-2,
                legend style={font=\tiny},
                xmin=0,
                xtick distance=4000,
                axis lines=left,
                grid=both
            ]
                \addplot+[mark=*, mark size=1pt, thick, red] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/pde_continuity_optimizer_1.csv};
                \addlegendentry{Adam}
                \addplot+[mark=*, mark size=1pt, thick, green] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/pde_continuity_optimizer_2.csv};
                \addlegendentry{Adagrad}
                \addplot+[mark=*, mark size=1pt, thick, blue] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/pde_continuity_optimizer_4.csv};
                \addlegendentry{ASGD}
            \end{axis}
        \end{tikzpicture}
        \caption{Уравнение непрерывности}
        \label{fig:pde_continuity_optimizer}
    \end{subfigure}
    \caption{Функция потерь для уравнений Навье-Стокса \eqref{eq:navier_stockes} при разных оптимизаторах}
    \label{fig:pde_loss_optimizer}
\end{figure}

Для уравнений Навье-Стокса можно заметить рост функции потерь для оптимизатора
ASGD (рис. \ref{fig:pde_ux_optimizer} и \ref{fig:pde_continuity_optimizer}).
Таким образом происходит процесс поиска глобального минимума. Дело в том,
что функция потерь для верхней границы много больше, чем для уравнений Навье-Стокса
($10^{-0.8}$ против $10^{-2.9}$). Оптимизатор пытается выбраться из локального 
минимума, где решение стремится к нулевому в силу своей корректности с точки
зрения уравнений Навье-Стокса. Что касательно уравнения для скорости $u_y$
(рис. \ref{fig:pde_uy_optimizer}), график оптимизатора ASGD остается
практически неизменным, что опять же соответствует нулевому решению.
Остальные оптимизаторы имеют поведение схожее с поведением на границах.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            width=0.6\textwidth,
            ymode=log,
            xmin=0,
            xtick distance=2000,
            axis lines=left,
            grid=both,
        ]
            \addplot+[mark=*, mark size=1pt, thick, red] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/total_loss_optimizer_1.csv};
            \addlegendentry{Adam}
            \addplot+[mark=*, mark size=1pt, thick, green] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/total_loss_optimizer_2.csv};
            \addlegendentry{Adagrad}
            \addplot+[mark=*, mark size=1pt, thick, blue] table[x=step, y=value, col sep=comma]{data/couette_abu/loss/total_loss_optimizer_4.csv};
            \addlegendentry{ASGD}
        \end{axis}
    \end{tikzpicture}
    \caption{Итоговая функция потерь при разных оптимизаторах}
    \label{fig:total_loss_optimizer}
\end{figure}

Итого на суммарной функции потерь (рис. \ref{fig:total_loss_optimizer}),
оптимизатор Adam имеет наименьшую функцию потерь.